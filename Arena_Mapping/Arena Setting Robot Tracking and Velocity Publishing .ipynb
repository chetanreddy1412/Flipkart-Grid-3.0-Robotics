{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Code (Run From Here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Steps (used to get the start points, checkpoints and destination points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from math import atan2, cos, sin, sqrt, pi\n",
    "\n",
    "def get_start_points(w, h):\n",
    "    a = h / 10\n",
    "    b1 = np.array([8.5*a, 0.5*a])\n",
    "    b2 = b1 + np.array([a, 0])\n",
    "    b3 = b2 + np.array([a, 0])\n",
    "    b4 = b3 + np.array([a, 0])\n",
    "    return [\n",
    "        [int(x) for x in b1],\n",
    "        [int(x) for x in b2],\n",
    "        [int(x) for x in b3],\n",
    "        [int(x) for x in b4]\n",
    "    ]\n",
    "\n",
    "\n",
    "def get_destinations(w, h):\n",
    "    a = h / 10\n",
    "    d1 = np.array([3*a//2, 17*a//2], np.uint8)\n",
    "    d2 = d1 + np.array([0, a])\n",
    "    d3 = d2 + np.array([17*a, 0])\n",
    "    d4 = d3 + np.array([0, -a])\n",
    "    return [\n",
    "        [int(x) for x in d1],\n",
    "        [int(x) for x in d2],\n",
    "        [int(x) for x in d3],\n",
    "        [int(x) for x in d4]\n",
    "    ]\n",
    "\n",
    "\n",
    "def get_checkpoints(w, h):\n",
    "    a = h / 10\n",
    "    c1 = np.array([17*a//2, 17*a//2], np.uint8)\n",
    "    c2 = c1 + np.array([a, a])\n",
    "    c3 = c2 + np.array([a, 0])\n",
    "    c4 = c3 + np.array([a, -a])\n",
    "    return [\n",
    "        [int(x) for x in c1],\n",
    "        [int(x) for x in c2],\n",
    "        [int(x) for x in c3],\n",
    "        [int(x) for x in c4]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HSV Calculator (used to detect the two markers on the bot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HSV FINDER TOOL\n",
    "\n",
    "def nothing(x):\n",
    "    pass\n",
    "\n",
    "def create_Trackbars(trackbarwindow = 'Controls',num_masks=1):\n",
    "    \n",
    "    lower = [0,0,0]\n",
    "    upper = [180,255,255]\n",
    "    #HUE\n",
    "    cv2.createTrackbar('lh_mask1',trackbarwindow,lower[0],179,nothing)\n",
    "    cv2.createTrackbar('uh_mask1',trackbarwindow,upper[0],179,nothing)\n",
    "    \n",
    "    #Saturation\n",
    "    cv2.createTrackbar('ls_mask1',trackbarwindow,lower[1],255,nothing)\n",
    "    cv2.createTrackbar('us_mask1',trackbarwindow,upper[1],255,nothing)\n",
    "    \n",
    "    #Value\n",
    "    cv2.createTrackbar('lv_mask1',trackbarwindow,lower[2],255,nothing)\n",
    "    cv2.createTrackbar('uv_mask1',trackbarwindow,upper[2],255,nothing)\n",
    "    \n",
    "    #Same for Mask2\n",
    "    if num_masks==2:\n",
    "        cv2.createTrackbar('lh_mask2',trackbarwindow,lower[0],179,nothing)\n",
    "        cv2.createTrackbar('uh_mask2',trackbarwindow,lower[0],179,nothing)    \n",
    "\n",
    "        cv2.createTrackbar('ls_mask2',trackbarwindow,lower[1],255,nothing)\n",
    "        cv2.createTrackbar('us_mask2',trackbarwindow,lower[1],255,nothing)\n",
    "\n",
    "        cv2.createTrackbar('lv_mask2',trackbarwindow,lower[2],255,nothing)\n",
    "        cv2.createTrackbar('uv_mask2',trackbarwindow,lower[2],255,nothing)\n",
    "    \n",
    "    cv2.createTrackbar('save',trackbarwindow,0,1,nothing)\n",
    "    #cv2.createTrackbar('mode',trackbarwindow,0,3,nothing)\n",
    "\n",
    "def get_mask_3d(mask_number,hsv_img,trackbarwindow):#Here, 3d indicates that the shape of the mask is a tuple\n",
    "    \n",
    "    lh = cv2.getTrackbarPos('lh_mask{}'.format(mask_number),trackbarwindow)\n",
    "    uh = cv2.getTrackbarPos('uh_mask{}'.format(mask_number),trackbarwindow)\n",
    "    ls = cv2.getTrackbarPos('ls_mask{}'.format(mask_number),trackbarwindow)\n",
    "    us = cv2.getTrackbarPos('us_mask{}'.format(mask_number),trackbarwindow)\n",
    "    lv = cv2.getTrackbarPos('lv_mask{}'.format(mask_number),trackbarwindow)\n",
    "    uv = cv2.getTrackbarPos('uv_mask{}'.format(mask_number),trackbarwindow)\n",
    "    \n",
    "    lower = np.array([lh,ls,lv])\n",
    "    upper = np.array([uh,us,uv])\n",
    "    mask = cv2.inRange(hsv_img,lower,upper)\n",
    "    mask_3d = cv2.cvtColor(mask,cv2.COLOR_GRAY2BGR)    \n",
    "    \n",
    "    return mask_3d,[lower,upper]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hsv_limits(img,window_name=\"Controls\"):\n",
    "    cv2.namedWindow(window_name)\n",
    "    create_Trackbars(window_name,num_masks=1)\n",
    "    hsv = cv2.cvtColor(img,cv2.COLOR_BGR2HSV)   \n",
    "    while True:\n",
    "        mask,hsv_limits = get_mask_3d(1,hsv,window_name)\n",
    "        mask_gray = cv2.cvtColor(mask,cv2.COLOR_BGR2GRAY)\n",
    "        contours,h = cv2.findContours(mask_gray.copy(),cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        if contours:\n",
    "            max_area_cnt = max(contours, key = cv2.contourArea)\n",
    "            peri = cv2.arcLength(max_area_cnt, True)\n",
    "            approx = cv2.approxPolyDP(max_area_cnt, 0.02 * peri, True)\n",
    "            pnts = approx.squeeze()\n",
    "\n",
    "            img_contours = img.copy()\n",
    "            cv2.drawContours(img_contours,[pnts],-1,(0,0,255),15)\n",
    "\n",
    "        else:\n",
    "            img_contours = img.copy()\n",
    "\n",
    "        img_and_mask = np.hstack((img,mask)) \n",
    "        img_and_mask = cv2.resize(img_and_mask,None,fx = 0.4,fy=0.4)\n",
    "        cv2.imshow(window_name,img_and_mask)\n",
    "        key = cv2.waitKey(10)\n",
    "        if key == ord('s'):\n",
    "            break\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    return hsv_limits # [lower,upper]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding centroids of the markers and finding their orientations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns the centroid of the detected marker on the bot which is later used to find the orientation\n",
    "def get_centroid(bot_img,hsv_limits,draw_bb,marker_color):\n",
    "    hsv_bot_img = cv2.cvtColor(bot_img, cv2.COLOR_BGR2HSV)\n",
    "    mask = cv2.inRange(hsv_bot_img,hsv_limits[0],hsv_limits[1])\n",
    "    contours,h = cv2.findContours(mask.copy(),cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
    "    marker_detected = False\n",
    "    if contours:\n",
    "        max_area_cnt = max(contours, key = cv2.contourArea)\n",
    "        M = cv2.moments(max_area_cnt)\n",
    "        if M['m00']==0:\n",
    "            marker_detected = False\n",
    "        else: \n",
    "            cx = int(M['m10']/M['m00'])\n",
    "            cy = int(M['m01']/M['m00'])\n",
    "            if draw_bb:\n",
    "                bot_img = cv2.drawContours(bot_img,[max_area_cnt],-1,marker_color,2)\n",
    "            marker_detected = True\n",
    "    \n",
    "    if marker_detected == True:\n",
    "        return (cx,cy)\n",
    "    \n",
    "    else: \n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angle(img,previous_angle,hsv_limits1,hsv_limits,draw_bb = True):\n",
    "    c1 = get_centroid(img,hsv_limits1,draw_bb,marker_color=(255,0,0))\n",
    "    c2 = get_centroid(img,hsv_limits2,draw_bb,marker_color=(0,255,255))\n",
    "    if c1 == -1 or c2 == -1:\n",
    "        angle = previous_angle\n",
    "\n",
    "    else:\n",
    "        angle = round((atan2(c1[1]-c2[1],c2[0]-c1[0])*180/pi))\n",
    "\n",
    "    return angle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Command Function to give commands to the bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import atan2,pi\n",
    "def getcommand(loc, bot_orientation, dst):\n",
    "    x,y = loc\n",
    "    x1,y1 = dst\n",
    "    target_orientation = (atan2((y-y1),(x1-x)))*180/pi\n",
    "    diff_angle =  target_orientation-bot_orientation\n",
    "    \n",
    "    #converting negative difference in angle to positive\n",
    "    if diff_angle<0:\n",
    "        diff_angle += 360 \n",
    "\n",
    "    #if the bot's orientation is within 20 degrees of the required orientation, we go straight\n",
    "    if diff_angle <= 20:\n",
    "        return 'F'\n",
    "    \n",
    "    #the difference angle is measured from the bots orientation vector to the destination direction vector\n",
    "    #if it is within 180 degrees, the command is to take a left\n",
    "    elif 20<diff_angle < 180:\n",
    "        return 'L'\n",
    "    else:\n",
    "        return 'R'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Used to warp an image when 4 points are given (used when cropping the arena)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_points(pts):\n",
    "    #parameters: pts 4x2 np.ndarray\n",
    "    #returns: 4x2 np.ndarray\n",
    "    \n",
    "    # The function initialises a list of coordinates that will be ordered\n",
    "    # such that the first entry in the list is the top-left,\n",
    "    # the second entry is the top-right, the third is the\n",
    "    # bottom-right, and the fourth is the bottom-left\n",
    "#*************************************************************#\n",
    "\n",
    "\n",
    "    rect = np.zeros((4, 2), dtype = \"float32\")\n",
    "    # the top-left point will have the smallest sum, whereas\n",
    "    # the bottom-right point will have the largest sum\n",
    "    s = pts.sum(axis = 1)\n",
    "    rect[0] = pts[np.argmin(s)]\n",
    "    rect[2] = pts[np.argmax(s)]\n",
    "    # now, compute the difference between the points, the\n",
    "    # top-right point will have the smallest difference,\n",
    "    # whereas the bottom-left will have the largest difference\n",
    "    diff = np.diff(pts, axis = 1)\n",
    "    rect[1] = pts[np.argmin(diff)]\n",
    "    rect[3] = pts[np.argmax(diff)]\n",
    "    # return the ordered coordinates\n",
    "    return rect\n",
    "\n",
    "def four_point_transform(image, pts):\n",
    "#returns a warped image\n",
    "#parameters: image (np.ndarray) and the 4 pts np.ndarray of size (4,2)\n",
    "\n",
    "    # obtain a consistent order of the points and unpack them\n",
    "    # individually\n",
    "    rect = order_points(pts)\n",
    "    (tl, tr, br, bl) = rect\n",
    "    # compute the width of the new image, which will be the\n",
    "    # maximum distance between bottom-right and bottom-left\n",
    "    # x-coordiates or the top-right and top-left x-coordinates\n",
    "    widthA = np.sqrt(((br[0] - bl[0]) ** 2) + ((br[1] - bl[1]) ** 2))\n",
    "    widthB = np.sqrt(((tr[0] - tl[0]) ** 2) + ((tr[1] - tl[1]) ** 2))\n",
    "    maxWidth = max(int(widthA), int(widthB))\n",
    "    # compute the height of the new image, which will be the\n",
    "    # maximum distance between the top-right and bottom-right\n",
    "    # y-coordinates or the top-left and bottom-left y-coordinates\n",
    "    heightA = np.sqrt(((tr[0] - br[0]) ** 2) + ((tr[1] - br[1]) ** 2))\n",
    "    heightB = np.sqrt(((tl[0] - bl[0]) ** 2) + ((tl[1] - bl[1]) ** 2))\n",
    "    maxHeight = max(int(heightA), int(heightB))\n",
    "    # now that we have the dimensions of the new image, construct\n",
    "    # the set of destination points to obtain a \"birds eye view\",\n",
    "    # (i.e. top-down view) of the image, again specifying points\n",
    "    # in the top-left, top-right, bottom-right, and bottom-left\n",
    "    # order\n",
    "    dst = np.array([\n",
    "        [0, 0],\n",
    "        [maxWidth - 1, 0],\n",
    "        [maxWidth - 1, maxHeight - 1],\n",
    "    [0, maxHeight - 1]], dtype = \"float32\")\n",
    "    # compute the perspective transform matrix and then apply it\n",
    "    M = cv2.getPerspectiveTransform(rect, dst)\n",
    "    warped = cv2.warpPerspective(image, M, (maxWidth, maxHeight))\n",
    "    # return the warped image\n",
    "    return warped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arena Locking Stage\n",
      "Arena Locking Done\n",
      "\n",
      "Drawing bounding boxes\n",
      "\n",
      "BOT Tracking...\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "L\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "L\n",
      "R\n",
      "L\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "L\n",
      "L\n",
      "L\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "L\n",
      "L\n",
      "L\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "L\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "L\n",
      "L\n",
      "L\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "R\n",
      "R\n",
      "R\n",
      "F\n",
      "F\n",
      "F\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "L\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "L\n",
      "L\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n"
     ]
    }
   ],
   "source": [
    "video1 = \"Moving Bot.mp4\"\n",
    "video2 = \"total_arcade2.mp4\"\n",
    "video3 = \"total_arcade3.mp4\"\n",
    "video4 = \"total_arcade_endless.mp4\"\n",
    "\n",
    "cap = cv2.VideoCapture(video1)\n",
    "   \n",
    "# Check if camera opened successfully\n",
    "if (cap.isOpened()== False): \n",
    "    print(\"Error opening video  file\")\n",
    "\n",
    "cv2.namedWindow('Frame')\n",
    "\n",
    "grid_coordinates = []\n",
    "\n",
    "def save_points(event, x, y, flags, params):\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        grid_coordinates.append((x,y))\n",
    "\n",
    "################################# Locking and Cropping the Arena ###########################\n",
    "\n",
    "print(\"Arena Locking Stage\")\n",
    "while(cap.isOpened()):\n",
    "      \n",
    "  # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    if ret == True:\n",
    "   \n",
    "    # Display the resulting frame\n",
    "        if len(grid_coordinates)!=0:\n",
    "            for point in grid_coordinates:\n",
    "                cv2.circle(frame, point, radius=2, color=(0,0,255), thickness=4)\n",
    "        cv2.setMouseCallback('Frame', save_points)\n",
    "        cv2.imshow(\"Frame\",frame)\n",
    "\n",
    "    \n",
    "        key = cv2.waitKey(25)\n",
    "        #clear the points if c is pressed\n",
    "        if key == ord('c'):\n",
    "            grid_coordinates =[]\n",
    "        if key == ord('f'):\n",
    "            w = frame.shape[1]\n",
    "            h = frame.shape[0]\n",
    "            grid_coordinates=[(0,0),(0,h),(w,0),(w,h)]\n",
    "            break\n",
    "        if key & 0xFF == ord('s'):\n",
    "            break\n",
    "        if key &0xFF == ord('p'): ##use this only for saving a photo after choosing 4 points \n",
    "            cv2.imwrite(\"grid_cropped{}.jpeg\".format(np.random.randint(0,100)),four_point_transform(frame,np.array(grid_coordinates)))\n",
    "            \n",
    "   \n",
    "  # Break the loop\n",
    "    else: \n",
    "        break\n",
    "cv2.destroyAllWindows()\n",
    "print(\"Arena Locking Done\\n\")\n",
    "\n",
    "################################### Selecting BOTS to be tracked #############################\n",
    "if len(grid_coordinates)!=4:\n",
    "    print(\"Didnt choose 4 points correctly. Choosing entire frame\")\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "else:\n",
    "    print(\"Drawing bounding boxes\")\n",
    "    TrDict = {'csrt': cv2.legacy.TrackerCSRT_create,\n",
    "             'kcf' : cv2.legacy.TrackerKCF_create,\n",
    "              }\n",
    "    trackers = cv2.legacy.MultiTracker_create()\n",
    "\n",
    "    ret,frame_roi = cap.read()\n",
    "    frame = four_point_transform(frame_roi,np.array(grid_coordinates))\n",
    "    bot_largest_dim = 0\n",
    "    fraction = 0.9\n",
    "    num_bots=1\n",
    "    for i in range(num_bots):\n",
    "\n",
    "        if ret:\n",
    "            cv2.imshow('Frame',frame)\n",
    "            bbi = cv2.selectROI('Frame',frame)\n",
    "            bot_largest_dim = max(bbi[2:])\n",
    "            tracker_i = TrDict['csrt']()\n",
    "            trackers.add(tracker_i,frame,bbi)\n",
    "            cv2.waitKey(1)\n",
    "        else:\n",
    "            print(\"Cap not reading in ROI Selection Stage\")\n",
    "            break\n",
    "    bot_largest_dim = int(fraction*bot_largest_dim) #The extra region around the bot for detection of the bot\n",
    "    \n",
    "################################ HSV LIMITS SELECTION ##########################\n",
    "    \n",
    "    bot_img_t0 = frame[bbi[1]:bbi[1]+bbi[3],bbi[0]:bbi[0]+bbi[2]]\n",
    "    bot_img_t0 = frame[:,:]\n",
    "    cv2.imshow(\"bot_img_t0\",bot_img_t0)\n",
    "    hsv_limits1 = get_hsv_limits(bot_img_t0,\"Finding HSV Limits of Marker 1\")\n",
    "    hsv_limits2 = get_hsv_limits(bot_img_t0,\"Finding HSV Limits of Marker 2\")\n",
    "    \n",
    "############################# Tracking and Velocity Publishing ##############################\n",
    "    print(\"\\nBOT Tracking...\")\n",
    "    angles = [-90 for i in range(num_bots)]\n",
    "    bot_imgs = [0 for i in range(num_bots)]\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Not able to read in Tracking Stage\")\n",
    "            break\n",
    "        frame = four_point_transform(frame,np.array(grid_coordinates))\n",
    "        #frame_c = frame.copy()\n",
    "        #cv2.imshow(\"orig\",frame_c)\n",
    "        (success, boxes) = trackers.update(frame)\n",
    "\n",
    "        height,width,_ = frame.shape\n",
    "        \n",
    "        for i in range(len(boxes)):\n",
    "            box = boxes[i]\n",
    "            (x, y, w, h) = [int(a) for a in box]\n",
    "            bot_centroid = (int(x + w/2), int(y + h/2))\n",
    "            \n",
    "            search_rect = [bot_centroid[0],bot_centroid[1],\n",
    "                               bot_centroid[0]+2*bot_largest_dim,bot_centroid[1]+2*bot_largest_dim]\n",
    "            \n",
    "            frame_padded = np.stack([np.pad(frame[:,:,c], bot_largest_dim,\n",
    "                        mode='constant', constant_values=0) for c in range(3)], axis=2)\n",
    "            bot_imgs[i] = frame_padded[search_rect[1]:search_rect[3],search_rect[0]:search_rect[2]]\n",
    "\n",
    "            angles[i] = get_angle(bot_imgs[i],angles[i],hsv_limits1,hsv_limits2,draw_bb = True)\n",
    "            \n",
    "            destination_point = (int(width/2),35)\n",
    "            cv2.putText(frame,str(destination_point),destination_point,cv2.FONT_HERSHEY_SIMPLEX,1,(0,255,0),2)\n",
    "            cv2.circle(frame,destination_point , radius=4, color=(0,0,255), thickness=4)\n",
    "            \n",
    "            #cv2.putText(frame,str(angles[i]),bot_centroid,cv2.FONT_HERSHEY_SIMPLEX,1,(0,0,255),2)\n",
    "            cv2.circle(frame,bot_centroid , radius=4, color=(0,0,150), thickness=4)\n",
    "            command = getcommand(bot_centroid,angles[i],destination_point)\n",
    "            cv2.putText(frame,str(command),(bot_centroid[0]+10,bot_centroid[1]+10),cv2.FONT_HERSHEY_SIMPLEX,1,(0,150,0),2)\n",
    "            \n",
    "\n",
    "            print(command)\n",
    "            ############CODE to Send to the server#############\n",
    "            '''\n",
    "            \n",
    "            \n",
    "            \n",
    "            '''\n",
    "            \n",
    "        bot_img = np.hstack(bot_imgs) \n",
    "        cv2.imshow('Frame',frame)\n",
    "        cv2.imshow('bot_img',bot_img)\n",
    "        #cv2.imshow('bot_with_triangle',img_with_triangle)\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other redundant/supplementary code (below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Reading Code And Saving Snapshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames per second :  30.0 FPS\n",
      "Frame count :  988.0\n",
      "Frames per second :  30.0 FPS\n"
     ]
    }
   ],
   "source": [
    "import cv2 \n",
    "import numpy as np\n",
    "import time\n",
    "# Create a video capture object, in this case we are reading the video from a file\n",
    "vid_capture = cv2.VideoCapture(\"total_arcade_endless.mp4\")\n",
    "\n",
    "if (vid_capture.isOpened() == False):\n",
    "    print(\"Error opening the video file\")\n",
    "else:\n",
    "\n",
    "    fps = vid_capture.get(5)\n",
    "    print('Frames per second : ', fps,'FPS')\n",
    "\n",
    "# Get frame count\n",
    "    # You can replace 7 with CAP_PROP_FRAME_COUNT as well, they are enumerations\n",
    "    frame_count = vid_capture.get(7)\n",
    "    print('Frame count : ', frame_count)\n",
    "    vid_capture.set(cv2.CAP_PROP_FPS, 1)\n",
    "    fps = vid_capture.get(5)\n",
    "    print('Frames per second : ', fps,'FPS')\n",
    "while(vid_capture.isOpened()):\n",
    "    # vid_capture.read() methods returns a tuple, first element is a bool \n",
    "    # and the second is frame\n",
    "    ret, frame = vid_capture.read()\n",
    "    if ret == True:\n",
    "        cv2.imshow('Frame',frame)\n",
    "        # 20 is in milliseconds, try to increase the value, say 50 and observe\n",
    "        key = cv2.waitKey(20)\n",
    "        \n",
    "       # time.sleep(0.1)\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "        if key == ord('s'):\n",
    "            cv2.imwrite(\"triangle{}.jpeg\".format(np.random.randint(0,100)),frame)\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Release the video capture object\n",
    "vid_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arena Locking Tool and Mapping (Ignore this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HSV FINDER TOOL\n",
    "\n",
    "def nothing(x):\n",
    "    pass\n",
    "\n",
    "def create_Trackbars(trackbarwindow = 'Controls'):\n",
    "    \n",
    "    lower = [0,0,0]\n",
    "    upper = [180,255,255]\n",
    "    #HUE\n",
    "    cv2.createTrackbar('lh_mask1',trackbarwindow,lower[0],179,nothing)\n",
    "    cv2.createTrackbar('uh_mask1',trackbarwindow,upper[0],179,nothing)\n",
    "    \n",
    "    #Saturation\n",
    "    cv2.createTrackbar('ls_mask1',trackbarwindow,lower[1],255,nothing)\n",
    "    cv2.createTrackbar('us_mask1',trackbarwindow,upper[1],255,nothing)\n",
    "    \n",
    "    #Value\n",
    "    cv2.createTrackbar('lv_mask1',trackbarwindow,lower[2],255,nothing)\n",
    "    cv2.createTrackbar('uv_mask1',trackbarwindow,upper[2],255,nothing)\n",
    "    \n",
    "    #Same for Mask2\n",
    "    cv2.createTrackbar('lh_mask2',trackbarwindow,lower[0],179,nothing)\n",
    "    cv2.createTrackbar('uh_mask2',trackbarwindow,lower[0],179,nothing)    \n",
    "    \n",
    "    cv2.createTrackbar('ls_mask2',trackbarwindow,lower[1],255,nothing)\n",
    "    cv2.createTrackbar('us_mask2',trackbarwindow,lower[1],255,nothing)\n",
    "    \n",
    "    cv2.createTrackbar('lv_mask2',trackbarwindow,lower[2],255,nothing)\n",
    "    cv2.createTrackbar('uv_mask2',trackbarwindow,lower[2],255,nothing)\n",
    "    \n",
    "    cv2.createTrackbar('save',trackbarwindow,0,1,nothing)\n",
    "    #cv2.createTrackbar('mode',trackbarwindow,0,3,nothing)\n",
    "\n",
    "def get_mask_3d(mask_number,hsv_img,trackbarwindow):#Here, 3d indicates that the shape of the mask is a tuple\n",
    "    \n",
    "    lh = cv2.getTrackbarPos('lh_mask{}'.format(mask_number),trackbarwindow)\n",
    "    uh = cv2.getTrackbarPos('uh_mask{}'.format(mask_number),trackbarwindow)\n",
    "    ls = cv2.getTrackbarPos('ls_mask{}'.format(mask_number),trackbarwindow)\n",
    "    us = cv2.getTrackbarPos('us_mask{}'.format(mask_number),trackbarwindow)\n",
    "    lv = cv2.getTrackbarPos('lv_mask{}'.format(mask_number),trackbarwindow)\n",
    "    uv = cv2.getTrackbarPos('uv_mask{}'.format(mask_number),trackbarwindow)\n",
    "    \n",
    "    lower = np.array([lh,ls,lv])\n",
    "    upper = np.array([uh,us,uv])\n",
    "    mask = cv2.inRange(hsv_img,lower,upper)\n",
    "    mask_3d = cv2.cvtColor(mask,cv2.COLOR_GRAY2BGR)    \n",
    "    \n",
    "    return mask_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_contours(img,contours,thickness=3):\n",
    "    img_c = img.copy()\n",
    "    cv2.drawContours(img_c,contours,-1,(255,0,0),thickness)\n",
    "    plt.imshow(img_c,cmap='gray')\n",
    "    plt.show()\n",
    "    return img_c\n",
    "\n",
    "def get_points_for_warping(qmask):\n",
    "    \n",
    "    qmask_gray = cv2.cvtColor(qmask,cv2.COLOR_BGR2GRAY)\n",
    "    contours,h = cv2.findContours(qmask_gray.copy(),cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    max_area_cnt = max(contours, key = cv2.contourArea)\n",
    "    peri = cv2.arcLength(max_area_cnt, True)\n",
    "    approx = cv2.approxPolyDP(max_area_cnt, 0.02 * peri, True)\n",
    "    pnts = approx.squeeze()\n",
    "    \n",
    "    if pnts.shape !=(4,2):\n",
    "        visualise_contours(img,[contours])\n",
    "    return pnts,contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_points(pts):\n",
    "    #parameters: pts 4x2 np.ndarray\n",
    "    #returns: 4x2 np.ndarray\n",
    "    \n",
    "    # The function initialises a list of coordinates that will be ordered\n",
    "    # such that the first entry in the list is the top-left,\n",
    "    # the second entry is the top-right, the third is the\n",
    "    # bottom-right, and the fourth is the bottom-left\n",
    "#*************************************************************#\n",
    "\n",
    "\n",
    "    rect = np.zeros((4, 2), dtype = \"float32\")\n",
    "    # the top-left point will have the smallest sum, whereas\n",
    "    # the bottom-right point will have the largest sum\n",
    "    s = pts.sum(axis = 1)\n",
    "    rect[0] = pts[np.argmin(s)]\n",
    "    rect[2] = pts[np.argmax(s)]\n",
    "    # now, compute the difference between the points, the\n",
    "    # top-right point will have the smallest difference,\n",
    "    # whereas the bottom-left will have the largest difference\n",
    "    diff = np.diff(pts, axis = 1)\n",
    "    rect[1] = pts[np.argmin(diff)]\n",
    "    rect[3] = pts[np.argmax(diff)]\n",
    "    # return the ordered coordinates\n",
    "    return rect\n",
    "\n",
    "def four_point_transform(image, pts):\n",
    "#returns a warped image\n",
    "#parameters: image (np.ndarray) and the 4 pts np.ndarray of size (4,2)\n",
    "\n",
    "    # obtain a consistent order of the points and unpack them\n",
    "    # individually\n",
    "    rect = order_points(pts)\n",
    "    (tl, tr, br, bl) = rect\n",
    "    # compute the width of the new image, which will be the\n",
    "    # maximum distance between bottom-right and bottom-left\n",
    "    # x-coordiates or the top-right and top-left x-coordinates\n",
    "    widthA = np.sqrt(((br[0] - bl[0]) ** 2) + ((br[1] - bl[1]) ** 2))\n",
    "    widthB = np.sqrt(((tr[0] - tl[0]) ** 2) + ((tr[1] - tl[1]) ** 2))\n",
    "    maxWidth = max(int(widthA), int(widthB))\n",
    "    # compute the height of the new image, which will be the\n",
    "    # maximum distance between the top-right and bottom-right\n",
    "    # y-coordinates or the top-left and bottom-left y-coordinates\n",
    "    heightA = np.sqrt(((tr[0] - br[0]) ** 2) + ((tr[1] - br[1]) ** 2))\n",
    "    heightB = np.sqrt(((tl[0] - bl[0]) ** 2) + ((tl[1] - bl[1]) ** 2))\n",
    "    maxHeight = max(int(heightA), int(heightB))\n",
    "    # now that we have the dimensions of the new image, construct\n",
    "    # the set of destination points to obtain a \"birds eye view\",\n",
    "    # (i.e. top-down view) of the image, again specifying points\n",
    "    # in the top-left, top-right, bottom-right, and bottom-left\n",
    "    # order\n",
    "    dst = np.array([\n",
    "        [0, 0],\n",
    "        [maxWidth - 1, 0],\n",
    "        [maxWidth - 1, maxHeight - 1],\n",
    "    [0, maxHeight - 1]], dtype = \"float32\")\n",
    "    # compute the perspective transform matrix and then apply it\n",
    "    M = cv2.getPerspectiveTransform(rect, dst)\n",
    "    warped = cv2.warpPerspective(image, M, (maxWidth, maxHeight))\n",
    "    # return the warped image\n",
    "    return warped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initiate_mapping(cap):\n",
    "    \n",
    "    if cap.isOpened()==False:\n",
    "        print('Could not access feed')\n",
    "        return\n",
    "    \n",
    "    trackbarwindow = 'Controls'\n",
    "    cv2.namedWindow(trackbarwindow)\n",
    "    create_Trackbars(trackbarwindow)\n",
    "    \n",
    "    pnts = np.array([[0,0],[0,1],[1,0],[1,1]]) \n",
    "    while True:\n",
    "\n",
    "        ret,img = cap.read()\n",
    "        hsv_img = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "        save = cv2.getTrackbarPos('save',trackbarwindow)\n",
    "\n",
    "\n",
    "        mask1 = get_mask_3d(1,hsv_img,trackbarwindow)\n",
    "        mask2 = get_mask_3d(2,hsv_img,trackbarwindow)\n",
    "        mask = cv2.bitwise_or(mask1,mask2)\n",
    "\n",
    "        mask1_and_mask2 = np.hstack((mask1,mask2))\n",
    "        img_and_mask = np.hstack((img,mask))\n",
    "        \n",
    "        stacked = np.vstack((img_and_mask,mask1_and_mask2))\n",
    "        stacked = cv2.resize(stacked,None,fx=0.25,fy=0.25)\n",
    "        cv2.imshow(\"Mask\",stacked)\n",
    "        \n",
    "        mask_gray = cv2.cvtColor(mask,cv2.COLOR_BGR2GRAY)\n",
    "        contours,h = cv2.findContours(mask_gray.copy(),cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        if contours:\n",
    "            max_area_cnt = max(contours, key = cv2.contourArea)\n",
    "            peri = cv2.arcLength(max_area_cnt, True)\n",
    "            approx = cv2.approxPolyDP(max_area_cnt, 0.02 * peri, True)\n",
    "            pnts = approx.squeeze()\n",
    "\n",
    "            img_contours = img.copy()\n",
    "            cv2.drawContours(img_contours,[pnts],-1,(0,0,255),15)\n",
    "            img_contours = cv2.resize(img_contours,None,fx = 0.7,fy=0.7)\n",
    "            cv2.imshow('Grid Recognition',img_contours)\n",
    "        \n",
    "        else:\n",
    "            cv2.imshow('Grid Recognition',img.copy())\n",
    "\n",
    "\n",
    "        warped = four_point_transform(img,pnts)\n",
    "        warped = cv2.resize(warped,None,fx = 0.7,fy=0.7)\n",
    "        cv2.imshow('Warped',warped)\n",
    "        key = cv2.waitKey(10)\n",
    "        \n",
    "        if save ==1:\n",
    "            mask_no = np.random.randint(1,10000)\n",
    "            cv2.imwrite('mask_{}.jpg'.format(mask_no),mask)\n",
    "            break\n",
    "        if key == ord('s'):\n",
    "            break\n",
    "            \n",
    "    cv2.destroyAllWindows()\n",
    "    while cap.isOpened():\n",
    "        ret,img = cap.read()\n",
    "        \n",
    "        warped = four_point_transform(img,pnts)\n",
    "        cv2.imshow('Grid',warped)\n",
    "        gray_warped = cv2.cvtColor(warped,cv2.COLOR_BGR2GRAY)\n",
    "        #ret,binary_warped = cv2.threshold(gray_warped,200,255,cv2.THRESH_BINARY)\n",
    "        gray_warped = cv2.resize(gray_warped, None,fx=2,fy=2)\n",
    "        cv2.imshow('Grid_Binary',gray_warped)\n",
    "        if cv2.waitKey(10)==ord('q'):\n",
    "            break\n",
    "        \n",
    "    cv2.destroyAllWindows()\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "cap.isOpened()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initiate_mapping(cap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_orientation_rough2(bot_img):\n",
    "    gray = cv2.cvtColor(bot_img, cv2.COLOR_BGR2GRAY)\n",
    "    blurred = cv2.GaussianBlur(gray, (7, 7), 0)\n",
    "\n",
    "    (T, mask) = cv2.threshold(blurred, 200, 255,cv2.THRESH_BINARY_INV)\n",
    "    contours,h = cv2.findContours(255-mask.copy(),cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
    "    img_contours = bot_img.copy()\n",
    "    if contours:\n",
    "        max_area_cnt = max(contours, key = cv2.contourArea)\n",
    "        peri = cv2.arcLength(max_area_cnt, True)\n",
    "        approx = cv2.approxPolyDP(max_area_cnt, 0.02 * peri, True)\n",
    "        pnts = approx.squeeze()\n",
    "        cv2.drawContours(img_contours,[pnts],-1,(0,0,255),3)\n",
    "\n",
    "   # angle = round(getOrientation(max_area_cnt, img_contours)*180/3.141)\n",
    "    return img_contours#,angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_orientation_rough1(bot_img):\n",
    "    gray = cv2.cvtColor(bot_img, cv2.COLOR_BGR2GRAY)\n",
    "    blurred = cv2.GaussianBlur(gray, (7, 7), 0)\n",
    "\n",
    "    (T, mask) = cv2.threshold(blurred, 200, 255,cv2.THRESH_BINARY_INV)\n",
    "    contours,h = cv2.findContours(255-mask.copy(),cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
    "    max_area_cnt = max(contours, key = cv2.contourArea)\n",
    "\n",
    "    img_c = bot_img.copy()\n",
    "    cv2.drawContours(img_c,max_area_cnt,-1,(255,0,0),2)\n",
    "    angle = round(getOrientation(max_area_cnt, img_c)*180/3.141)\n",
    "    return angle,img_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 22)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(int(w/2),int(h/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44, 17)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h,w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hsv_limits1 = get_hsv_limits(img,\"Finding HSV Limits of Marker 1\")\n",
    "hsv_limits2 = get_hsv_limits(img,\"Finding HSV Limits of Marker 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img)\n",
    "get_angle(img,0,hsv_limits1,hsv_limits2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret,thresh = cv2.threshold(img,127,255,0)\n",
    "print(thresh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "mask = cv2.inRange(hsv_img,)\n",
    "mask_3d = cv2.cvtColor(mask,cv2.COLOR_GRAY2BGR) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"grid_cropped5.jpeg\")\n",
    "cv2.imshow(\"img\",img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[21, 124], [21, 138], [269, 138], [269, 124]]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = img.shape[1]\n",
    "h = img.shape[0]\n",
    "\n",
    "get_start_points(w,h)\n",
    "get_checkpoints(w,h)\n",
    "get_destinations(w,h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, b in enumerate(get_start_points(w, h)):\n",
    "    x, y = b\n",
    "    cv2.circle(img, (x, y), 2, (255, 0, 0), -1)\n",
    "    cv2.putText(img, f's{i+1}', (x+1, y+1),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.25, (255, 0, 0), 1)\n",
    "\n",
    "for i, c in enumerate(get_checkpoints(w, h)):\n",
    "    x, y = c\n",
    "    cv2.circle(img, (x, y), 2, (0, 0, 255), -1)\n",
    "    cv2.putText(img, f'c{i+1}', (x+1, y+1),\n",
    "                 cv2.FONT_HERSHEY_SIMPLEX, 0.25, (0, 0, 255), 1)\n",
    "\n",
    "for i, d in enumerate(get_destinations(w, h)):\n",
    "    x, y = d\n",
    "    cv2.circle(img, (x, y), 2, (0, 255, 0), -1)\n",
    "    cv2.putText(img, f'd{i+1}', (x+1, y+1),\n",
    "               cv2.FONT_HERSHEY_SIMPLEX, 0.25, (0, 255, 0), 1)\n",
    "\n",
    "#cv2.imwrite('RESULT.png', img)\n",
    "cv2.imshow('Platform with points', img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'R'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getcommand((0,10),180,(10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pose Estimation using triangle marker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting Orientation from Bot Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from math import atan2, cos, sin, sqrt, pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pose(img,previous_angle,bot_centroid):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    blurred = cv2.GaussianBlur(gray, (7, 7), 0)\n",
    "    (T, mask) = cv2.threshold(blurred,230,255,cv2.THRESH_BINARY)\n",
    "\n",
    "    contours,h = cv2.findContours(mask.copy(),cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
    "    img_contours =  img.copy()\n",
    "    detected = False\n",
    "    if len(contours)!=0:\n",
    "        required_pnts=[]\n",
    "        max_area = 0\n",
    "        for cnt in contours:\n",
    "            peri = cv2.arcLength(cnt,True)\n",
    "            approx = cv2.approxPolyDP(cnt, 0.09 * peri, True)\n",
    "            area = cv2.contourArea(approx)\n",
    "            pnts = approx.squeeze()  #np.ndarray (n,1 size)\n",
    "            if len(pnts)==3:\n",
    "                if area<max_area:\n",
    "                    pass\n",
    "                else:\n",
    "                    detected = True\n",
    "                    required_pnts = pnts\n",
    "                    max_area = area\n",
    "        \n",
    "    \n",
    "    if detected:\n",
    "        cv2.drawContours(img_contours,[required_pnts],-1,(0,0,255),1)\n",
    "        max_norm = 0 \n",
    "        max_points = []\n",
    "        l1 = norm(required_pnts[0]-required_pnts[1])\n",
    "        l2 = norm(required_pnts[1]-required_pnts[2])\n",
    "        l3 = norm(required_pnts[2]-required_pnts[0])\n",
    "        if l1<=l2 and l1<=l3:\n",
    "            mid_point = (required_pnts[0]+required_pnts[1])/2\n",
    "            top_point = required_pnts[2]\n",
    "\n",
    "        if l2<=l1 and l2<=l3:\n",
    "            mid_point = (required_pnts[1]+required_pnts[2])/2\n",
    "            top_point = required_pnts[0]\n",
    "\n",
    "        if l3<=l1 and l3<=l2:\n",
    "            mid_point = (required_pnts[2]+required_pnts[0])/2\n",
    "            top_point = required_pnts[1]\n",
    "\n",
    "        mid_point = mid_point.astype(np.uint32) \n",
    "        angle = round(atan2(mid_point[1]-top_point[1],top_point[0]-mid_point[0])*180/pi)\n",
    "        centroid = (mid_point+top_point)/2\n",
    "        centroid = centroid.astype(np.uint32)\n",
    "    \n",
    "    else:\n",
    "        angle= previous_angle\n",
    "        centroid = bot_centroid\n",
    "    return angle,centroid,img_contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"5x5.png\")\n",
    "img_text = img.copy()\n",
    "plt.imshow(img)\n",
    "angle, centroid,img_contours = get_pose(img,0,(10,10))\n",
    "cv2.putText(img_text,str(angle),(10000,100),cv2.FONT_HERSHEY_SIMPLEX,1,(255,0,0),2)\n",
    "plt.imshow(img_contours)\n",
    "plt.figure()\n",
    "plt.imshow(img_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to Determine Orientation using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import atan2, cos, sin, sqrt, pi\n",
    "\n",
    "def drawAxis(img, p_, q_, color, scale):\n",
    "    p = list(p_)\n",
    "    q = list(q_)\n",
    " \n",
    "    ## [visualization1]\n",
    "    angle = atan2(p[1] - q[1], p[0] - q[0]) # angle in radians\n",
    "    hypotenuse = sqrt((p[1] - q[1]) * (p[1] - q[1]) + (p[0] - q[0]) * (p[0] - q[0]))\n",
    " \n",
    "    # Here we lengthen the arrow by a factor of scale\n",
    "    q[0] = p[0] - scale * hypotenuse * cos(angle)\n",
    "    q[1] = p[1] - scale * hypotenuse * sin(angle)\n",
    "    cv2.line(img, (int(p[0]), int(p[1])), (int(q[0]), int(q[1])), color, 3, cv2.LINE_AA)\n",
    " \n",
    "    # create the arrow hooks\n",
    "    p[0] = q[0] + 9 * cos(angle + pi / 4)\n",
    "    p[1] = q[1] + 9 * sin(angle + pi / 4)\n",
    "    cv2.line(img, (int(p[0]), int(p[1])), (int(q[0]), int(q[1])), color, 3, cv2.LINE_AA)\n",
    " \n",
    "    p[0] = q[0] + 9 * cos(angle - pi / 4)\n",
    "    p[1] = q[1] + 9 * sin(angle - pi / 4)\n",
    "    cv2.line(img, (int(p[0]), int(p[1])), (int(q[0]), int(q[1])), color, 3, cv2.LINE_AA)\n",
    "    ## [visualization1]\n",
    "\n",
    "def getOrientation(pts, img):\n",
    "    ## [pca]\n",
    "    # Construct a buffer used by the pca analysis\n",
    "    sz = len(pts)\n",
    "    data_pts = np.empty((sz, 2), dtype=np.float64)\n",
    "    for i in range(data_pts.shape[0]):\n",
    "        data_pts[i,0] = pts[i,0,0]\n",
    "        data_pts[i,1] = pts[i,0,1]\n",
    " \n",
    "    # Perform PCA analysis\n",
    "    mean = np.empty((0))\n",
    "    mean, eigenvectors, eigenvalues = cv2.PCACompute2(data_pts, mean)\n",
    " \n",
    "    # Store the center of the object\n",
    "    cntr = (int(mean[0,0]), int(mean[0,1]))\n",
    "     \n",
    "    ## [pca]\n",
    "    ## [visualization]\n",
    "    # Draw the principal components\n",
    "    #cv2.circle(img, cntr, 3, (255, 0, 255), 2)\n",
    "    p1 = (cntr[0] + 0.02 * eigenvectors[0,0] * eigenvalues[0,0], cntr[1] + 0.02 * eigenvectors[0,1] * eigenvalues[0,0])\n",
    "    p2 = (cntr[0] - 0.02 * eigenvectors[1,0] * eigenvalues[1,0], cntr[1] - 0.02 * eigenvectors[1,1] * eigenvalues[1,0])\n",
    "    drawAxis(img, cntr, p1, (255, 255, 0), 1)\n",
    "    drawAxis(img, cntr, p2, (0, 0, 255), 5)\n",
    " \n",
    "    angle = atan2(eigenvectors[0,1], eigenvectors[0,0]) # orientation in radians\n",
    "    ## [visualization]\n",
    "     # Label with the rotation angle\n",
    "  \n",
    "    #label = str(-int(np.rad2deg(angle)) - 90) + \" degrees\"\n",
    "    #textbox = cv2.rectangle(img, (cntr[0], cntr[1]-25), (cntr[0] + 250, cntr[1] + 10), (255,255,255), -1)\n",
    "    #cv2.putText(img, label, (cntr[0], cntr[1]), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,0), 1, cv2.LINE_AA)\n",
    " \n",
    "    return angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "   \n",
    "# Check if camera opened successfully\n",
    "if (cap.isOpened()== False): \n",
    "    print(\"Error opening video  file\")\n",
    "\n",
    "while(cap.isOpened()):\n",
    "      \n",
    "  # Capture frame-by-frame\n",
    "    ret, img = cap.read()\n",
    "    if ret == True:\n",
    "   \n",
    "    # Display the resulting frame\n",
    "        cv2.imshow(\"Frame\",img)\n",
    "        \n",
    "        # Convert image to grayscale\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Convert image to binary\n",
    "        _, bw = cv2.threshold(gray, 50, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
    "\n",
    "        # Find all the contours in the thresholded image\n",
    "        contours, _ = cv2.findContours(bw, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\n",
    "\n",
    "\n",
    "        for i, c in enumerate(contours):\n",
    "\n",
    "          # Calculate the area of each contour\n",
    "            area = cv2.contourArea(c)\n",
    "\n",
    "          # Ignore contours that are too small or too large\n",
    "            if area < 3700 or 100000 < area:\n",
    "                continue\n",
    "\n",
    "          # Draw each contour only for visualisation purposes\n",
    "            cv2.drawContours(img, contours, i, (0, 0, 255), 2)\n",
    "\n",
    "          # Find the orientation of each shape\n",
    "            #getOrientation(c, img)\n",
    "\n",
    "        cv2.imshow('Output Image', img)\n",
    "    \n",
    "        key = cv2.waitKey(25)\n",
    "        \n",
    "        if key & 0xFF == ord('q'):\n",
    "            break\n",
    "   \n",
    "  # Break the loop\n",
    "    else: \n",
    "        break\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image\n",
    "img = cv2.imread(\"bot.png\")\n",
    " \n",
    "# Was the image there?\n",
    "if img is None:\n",
    "    print(\"Error: File not found\")\n",
    "    exit(0)\n",
    "\n",
    "cv2.imshow('Input Image', img)\n",
    " \n",
    "# Convert image to grayscale\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    " \n",
    "# Convert image to binary\n",
    "_, bw = cv2.threshold(gray, 50, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
    " \n",
    "# Find all the contours in the thresholded image\n",
    "contours, _ = cv2.findContours(bw, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\n",
    " \n",
    "\n",
    "for i, c in enumerate(contours):\n",
    " \n",
    "  # Calculate the area of each contour\n",
    "    area = cv2.contourArea(c)\n",
    " \n",
    "  # Ignore contours that are too small or too large\n",
    "    if area < 3700 or 100000 < area:\n",
    "        continue\n",
    " \n",
    "  # Draw each contour only for visualisation purposes\n",
    "    cv2.drawContours(img, contours, i, (0, 0, 255), 2)\n",
    " \n",
    "  # Find the orientation of each shape\n",
    "    #getOrientation(c, img)\n",
    "\n",
    "cv2.imshow('Output Image', img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
